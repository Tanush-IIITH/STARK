
================================================================================
SHIFT-OR/BITAP IMPLEMENTATION - COMPLETE SETUP AND EXECUTION GUIDE
================================================================================

üì¶ ALL FILES CREATED:
==================
Core Algorithm Files:
  ‚úÖ shift_or_exact.py (Exact matching, ‚â§64 bp)
  ‚úÖ shift_or_approximate.py (Approximate matching, k=1,2,3, ‚â§64 bp)
  ‚úÖ shift_or_extended.py (Extended matching, >64 bp)
  ‚úÖ shift_or_utils.py (DNA utilities)

Benchmark Scripts:
  ‚úÖ benchmark_exact.py
  ‚úÖ benchmark_approximate.py
  ‚úÖ benchmark_extended.py

Testing & Examples:
  ‚úÖ test_quick.py (Validation tests)
  ‚úÖ example_usage.py (Usage demonstrations)

Documentation:
  ‚úÖ README.md (Main documentation)
  ‚úÖ ALGORITHM_EXPLANATION.md (Technical details)
  ‚úÖ requirements.txt (Dependencies)

üìù FILES YOU NEED TO CREATE:
==========================
1. Three Jupyter Notebooks (see templates below)
2. Optional: report.tex (LaTeX report)

================================================================================
STEP-BY-STEP EXECUTION INSTRUCTIONS
================================================================================

STEP 1: ORGANIZE FILES
=====================
Create directory structure:

mkdir -p Shift-Or-Bitap/results_exact
mkdir -p Shift-Or-Bitap/results_approximate  
mkdir -p Shift-Or-Bitap/results_extended

Move all created Python files to Shift-Or-Bitap/:
mv shift_or_*.py Shift-Or-Bitap/
mv benchmark_*.py Shift-Or-Bitap/
mv test_quick.py example_usage.py Shift-Or-Bitap/
mv README.md ALGORITHM_EXPLANATION.md requirements.txt Shift-Or-Bitap/

cd Shift-Or-Bitap/


STEP 2: INSTALL DEPENDENCIES
============================
pip install -r requirements.txt

This installs:
- matplotlib (for plots)
- numpy (for data processing)


STEP 3: VALIDATE INSTALLATION
=============================
Run quick tests to ensure everything works:

python test_quick.py

Expected output:
  ‚úÖ All exact matching tests passed!
  ‚úÖ All approximate matching tests passed!
  ‚úÖ All extended matching tests passed!
  ‚úÖ All error handling tests passed!
  ‚úÖ ALL TESTS PASSED SUCCESSFULLY!

If any test fails, check the error message and verify file integrity.


STEP 4: RUN EXAMPLE DEMONSTRATIONS
==================================
python example_usage.py

This demonstrates:
- Basic exact matching
- Approximate matching (k=1,2,3)
- Extended matching (>64 bp)
- Multiple pattern search
- Performance comparisons

Review the output to understand how to use the algorithms.


STEP 5: TEST ON SINGLE DATASET (OPTIONAL)
=========================================
Before running on all 90 datasets, test on one:

# Exact matching
python benchmark_exact.py \
  /path/to/your/dataset/GCF_XXXXXX.X/GCF_XXXXXX_genomic.fna \
  results_exact/GCF_XXXXXX.X

# Check output
ls -la results_exact/GCF_XXXXXX.X/
# Should see: 5 files (2 CSV, 2 JPG, 1 JSON)

# Approximate matching  
python benchmark_approximate.py \
  /path/to/your/dataset/GCF_XXXXXX.X/GCF_XXXXXX_genomic.fna \
  results_approximate/GCF_XXXXXX.X

# Check output
ls -la results_approximate/GCF_XXXXXX.X/
# Should see: 15 files (6 CSV, 6 JPG, 3 JSON) - 5 files per k value

# Extended matching
python benchmark_extended.py \
  /path/to/your/dataset/GCF_XXXXXX.X/GCF_XXXXXX_genomic.fna \
  results_extended/GCF_XXXXXX.X

# Check output  
ls -la results_extended/GCF_XXXXXX.X/
# Should see: 5 files (2 CSV, 2 JPG, 1 JSON)


STEP 6: CREATE JUPYTER NOTEBOOKS
================================

Create: shift_or_exact_nb.ipynb
-------------------------------
Content:

```python
import os
import sys
from benchmark_exact import run_benchmark_on_dataset

# Configure paths
DATASET_ROOT = "/path/to/your/90/datasets/folder"  # UPDATE THIS PATH
OUTPUT_ROOT = "results_exact"

# Get all dataset folders
datasets = sorted([d for d in os.listdir(DATASET_ROOT) 
                   if os.path.isdir(os.path.join(DATASET_ROOT, d))])

print(f"Found {len(datasets)} datasets")
print(f"Processing with Shift-Or exact matching (‚â§64 bp patterns)...")

# Process each dataset
for i, dataset_name in enumerate(datasets, 1):
    dataset_path = os.path.join(DATASET_ROOT, dataset_name)

    # Find .fna file
    fasta_file = None
    for file in os.listdir(dataset_path):
        if file.endswith('.fna') or file.endswith('.fasta'):
            fasta_file = os.path.join(dataset_path, file)
            break

    if not fasta_file:
        print(f"  {i}/{len(datasets)}: Skipping {dataset_name} - No FASTA file")
        continue

    # Run benchmark
    output_dir = os.path.join(OUTPUT_ROOT, dataset_name)
    print(f"\n{i}/{len(datasets)}: {dataset_name}")

    try:
        run_benchmark_on_dataset(fasta_file, output_dir)
    except Exception as e:
        print(f"  ‚úó Error: {e}")
        continue

print("\n" + "="*70)
print("‚úÖ EXACT MATCHING BENCHMARKS COMPLETED!")
print(f"Results saved to: {OUTPUT_ROOT}/")
print(f"Total datasets processed: {len([d for d in os.listdir(OUTPUT_ROOT)])} / {len(datasets)}")
print("="*70)
```


Create: shift_or_approximate_nb.ipynb
-------------------------------------
Content:

```python
import os
import sys
from benchmark_approximate import run_benchmark_on_dataset

# Configure paths
DATASET_ROOT = "/path/to/your/90/datasets/folder"  # UPDATE THIS PATH
OUTPUT_ROOT = "results_approximate"

# Get all dataset folders
datasets = sorted([d for d in os.listdir(DATASET_ROOT)
                   if os.path.isdir(os.path.join(DATASET_ROOT, d))])

print(f"Found {len(datasets)} datasets")
print(f"Processing with Shift-Or approximate matching (k=1,2,3)...")
print(f"This will generate 15 files per dataset (5 per k value)")


# Process each dataset
for i, dataset_name in enumerate(datasets, 1):
    dataset_path = os.path.join(DATASET_ROOT, dataset_name)

    # Find .fna file
    fasta_file = None
    for file in os.listdir(dataset_path):
        if file.endswith('.fna') or file.endswith('.fasta'):
            fasta_file = os.path.join(dataset_path, file)
            break

    if not fasta_file:
        print(f"  {i}/{len(datasets)}: Skipping {dataset_name} - No FASTA file")
        continue

    # Run benchmark (tests k=1,2,3)
    output_dir = os.path.join(OUTPUT_ROOT, dataset_name)
    print(f"\n{i}/{len(datasets)}: {dataset_name}")

    try:
        run_benchmark_on_dataset(fasta_file, output_dir, k_values=[1, 2, 3])
    except Exception as e:
        print(f"  ‚úó Error: {e}")
        continue

print("\n" + "="*70)
print("‚úÖ APPROXIMATE MATCHING BENCHMARKS COMPLETED!")
print(f"Results saved to: {OUTPUT_ROOT}/")
print(f"Total datasets processed: {len([d for d in os.listdir(OUTPUT_ROOT)])} / {len(datasets)}")
print("="*70)
```


Create: shift_or_extended_nb.ipynb
----------------------------------
Content:

```python
import os
import sys
from benchmark_extended import run_benchmark_on_dataset

# Configure paths
DATASET_ROOT = "/path/to/your/90/datasets/folder"  # UPDATE THIS PATH
OUTPUT_ROOT = "results_extended"

# Get all dataset folders
datasets = sorted([d for d in os.listdir(DATASET_ROOT)
                   if os.path.isdir(os.path.join(DATASET_ROOT, d))])

print(f"Found {len(datasets)} datasets")
print(f"Processing with Shift-Or extended matching (>64 bp patterns)...")

# Process each dataset
for i, dataset_name in enumerate(datasets, 1):
    dataset_path = os.path.join(DATASET_ROOT, dataset_name)

    # Find .fna file
    fasta_file = None
    for file in os.listdir(dataset_path):
        if file.endswith('.fna') or file.endswith('.fasta'):
            fasta_file = os.path.join(dataset_path, file)
            break

    if not fasta_file:
        print(f"  {i}/{len(datasets)}: Skipping {dataset_name} - No FASTA file")
        continue

    # Run benchmark
    output_dir = os.path.join(OUTPUT_ROOT, dataset_name)
    print(f"\n{i}/{len(datasets)}: {dataset_name}")

    try:
        run_benchmark_on_dataset(fasta_file, output_dir)
    except Exception as e:
        print(f"  ‚úó Error: {e}")
        continue

print("\n" + "="*70)
print("‚úÖ EXTENDED MATCHING BENCHMARKS COMPLETED!")
print(f"Results saved to: {OUTPUT_ROOT}/")
print(f"Total datasets processed: {len([d for d in os.listdir(OUTPUT_ROOT)])} / {len(datasets)}")
print("="*70)
```


STEP 7: RUN BENCHMARKS ON ALL DATASETS
======================================

Option A: Using Jupyter Notebook
--------------------------------
jupyter notebook

# Open and run each notebook in order:
1. shift_or_exact_nb.ipynb
   - Generates 450 files (90 datasets √ó 5 files)
   - Estimated time: ~30-60 minutes

2. shift_or_approximate_nb.ipynb  
   - Generates 1,350 files (90 datasets √ó 15 files)
   - Estimated time: ~2-4 hours (slower due to k=1,2,3)

3. shift_or_extended_nb.ipynb
   - Generates 450 files (90 datasets √ó 5 files)
   - Estimated time: ~30-60 minutes

Total files generated: ~2,250 files


Option B: Convert to Python Scripts and Run
-------------------------------------------
# Convert notebooks to .py files
jupyter nbconvert --to python shift_or_exact_nb.ipynb
jupyter nbconvert --to python shift_or_approximate_nb.ipynb
jupyter nbconvert --to python shift_or_extended_nb.ipynb

# Run scripts (can be done in background)
nohup python shift_or_exact_nb.py > exact.log 2>&1 &
nohup python shift_or_approximate_nb.py > approximate.log 2>&1 &
nohup python shift_or_extended_nb.py > extended.log 2>&1 &

# Monitor progress
tail -f exact.log
tail -f approximate.log  
tail -f extended.log


STEP 8: VERIFY RESULTS
======================
After completion, verify output structure:

# Count files
find results_exact -type f | wc -l        # Should be 450
find results_approximate -type f | wc -l  # Should be 1350
find results_extended -type f | wc -l     # Should be 450

# Check a sample dataset
ls -la results_exact/GCF_XXXXXX.X/
# Should show:
#   scaling_results.csv
#   scaling_time_memory.jpg
#   pattern_length_results.csv  
#   pattern_length_time_memory.jpg
#   multiple_patterns_summary.json

ls -la results_approximate/GCF_XXXXXX.X/
# Should show 15 files (k1, k2, k3 variants)

# Inspect a result file
cat results_exact/GCF_XXXXXX.X/scaling_results.csv
# Should show: slice_size, avg_time_s, peak_memory_mb


STEP 9: ANALYZE RESULTS
========================
Create analysis cells in notebooks:

```python
import pandas as pd
import glob
import matplotlib.pyplot as plt

# Aggregate all pattern length results
all_results = []
for csv_file in glob.glob("results_exact/*/pattern_length_results.csv"):
    df = pd.read_csv(csv_file)
    dataset = csv_file.split('/')[1]
    df['dataset'] = dataset
    all_results.append(df)

combined_df = pd.concat(all_results)

# Summary statistics
print("Average time by pattern length:")
print(combined_df.groupby('pattern_length')['time_seconds_mean'].mean())

print("\nAverage memory by pattern length:")
print(combined_df.groupby('pattern_length')['peak_memory_mb'].mean())

print("\nAverage bit operations by pattern length:")
print(combined_df.groupby('pattern_length')['bit_operations'].mean())

# Plot aggregated results
fig, ax = plt.subplots(figsize=(10, 6))
summary = combined_df.groupby('pattern_length')['time_seconds_mean'].mean()
ax.plot(summary.index, summary.values * 1000, 'o-')
ax.axvline(x=64, color='red', linestyle='--', label='64 bp limit')
ax.set_xlabel('Pattern Length (bp)')
ax.set_ylabel('Average Time (ms)')
ax.set_title('Shift-Or: Pattern Length vs Time (90 datasets)')
ax.legend()
ax.grid(True, alpha=0.3)
plt.savefig('summary_pattern_length.png', dpi=150)
plt.show()
```


STEP 10: GENERATE FINAL REPORT (OPTIONAL)
=========================================
Create summary document with:
- Algorithm description
- Implementation details  
- Performance results across 90 datasets
- Comparison with KMP
- Visualizations
- Conclusions

Use the template in report.tex or create your own.


================================================================================
TROUBLESHOOTING
================================================================================

Problem: "ModuleNotFoundError: No module named 'shift_or_exact'"
Solution: Ensure you're in the Shift-Or-Bitap/ directory when running scripts
         Or add: sys.path.append(os.path.dirname(os.path.abspath(__file__)))

Problem: "FileNotFoundError: FASTA file not found"
Solution: Check DATASET_ROOT path in notebooks points to correct location
         Verify .fna files exist in subdirectories

Problem: "MemoryError" during benchmarks
Solution: Reduce batch size or process datasets one at a time
         Consider running on machine with more RAM

Problem: Benchmarks running very slow
Solution: This is normal for approximate matching (k=1,2,3)
         Expected: ~2-4 hours for all 90 datasets with k=1,2,3
         Consider running overnight or in parallel

Problem: Some datasets fail to process
Solution: Check individual error messages in logs
         Common issues: corrupted FASTA, insufficient sequence length
         Skip problematic datasets and continue


================================================================================
EXPECTED RUNTIME
================================================================================

Hardware assumptions: Modern CPU, 16GB RAM, SSD

Single dataset:
- Exact matching: ~30-60 seconds
- Approximate matching (k=1,2,3): ~2-5 minutes
- Extended matching: ~30-60 seconds

All 90 datasets:
- Exact: ~45-90 minutes
- Approximate: ~3-7.5 hours (slower due to k=1,2,3)
- Extended: ~45-90 minutes
- TOTAL: ~5-10 hours for complete suite


================================================================================
FINAL CHECKLIST
================================================================================

Before running on all datasets:
‚òê All files created and organized
‚òê Dependencies installed (matplotlib, numpy)
‚òê test_quick.py passes all tests
‚òê Single dataset test successful
‚òê Jupyter notebooks created with correct DATASET_ROOT path
‚òê Sufficient disk space (~500 MB for results)
‚òê Sufficient time allocated (~10 hours)

After completion:
‚òê Verify file counts (450 + 1350 + 450 = 2250 files)
‚òê Spot-check results for correctness
‚òê Generate summary statistics
‚òê Create visualizations
‚òê Write report/documentation


================================================================================
CONTACT & SUPPORT
================================================================================

For questions or issues:
1. Check ALGORITHM_EXPLANATION.md for technical details
2. Review example_usage.py for usage patterns
3. Inspect error messages in logs
4. Verify FASTA file formats and integrity


================================================================================
SUCCESS!
================================================================================

Once all steps complete, you will have:
‚úÖ Complete Shift-Or/Bitap implementation (3 variants)
‚úÖ Comprehensive benchmarks on 90 DNA datasets
‚úÖ ~2,250 result files with metrics and visualizations
‚úÖ Detailed performance analysis
‚úÖ Comparison data with KMP algorithm
‚úÖ Publication-ready results

Good luck with your benchmarking! üöÄ
================================================================================
